# 实现基于HDR算法的实时VR视频播放器

小组成员以及分工：
* 16340023 陈明亮： 负责HDR算法的实现与效果优化，参考论文进行代码编写

* 16340024 陈铭涛： 负责HDR算法在视频处理上的防闪烁、降噪、提高帧率等优化

* 16340025 陈慕远： 负责实现实时VR视频播放器，使用HDR算法处理源视频



## 一、 HDR算法的基本实现与效果优化
1. 基于单张输入图像的HDR算法实现

   HDR算法的基本实现，实际上参考了*单幅图像的高动态范围图像生成方法*此篇论文，算法的主要步骤如下：

   * 在`YUV`颜色空间内，对原始图像的亮度分量进行反色调映射*Inverse Tone-Mapping*
   * 对处理之后的图像进行阈值图像的求取，同时进行高斯滤波，保留高光部分细节
   * 将前两部分处理得到的图像与色度分量图融合处理，在融合的同时对图像进行最后的色调调整和对比度优化
   * 进行噪点去除，同时针对暗部进行特殊处理


   整体的处理流程如下：

   ![img](../images/1.png)

   

   接下来对每个算法步骤进行详细介绍：

   * 反色调映射

     色调映射是将 HDR 图像数据映射成 LDR 图像数据, 并尽可能地在视觉上保留原来的视觉效果. 反色调映射则是色调映射的反变换, 用于增大图像的动态范围, 其对于 LDR 图像到 HDR 图像的转换同样非常关键. 

     ![img](../images/2.png)

     该式子即为反色调映射的核心算式，其中，$L_d(x,y)​$代表经过色调映射后 LDR 图像的像素点的值. 在映射过程中, 高亮度值的像素点可以近似地看作被 1/L 所量化压缩. 与此同时, 低亮度值的像素点可以看作被 1 量化压缩. 这使得高亮度值像素点在被压缩的同时, 低亮度值像素点的对比度得以保留.

     $L_{white}$ 决定了扩展函数的扩展曲线形状, 与映射后图像的对比度相关, 经过实验表明, 当取值较大时效果较好, 推荐采用 $L_{white}=L_{max}​$, 在限制伪像的同时提高对比度. 

     ![img](../images/3.png)


   * 高光区域处理

     最初获得的 LDR 图像在高光区域或暗区域往往存在着细节丢失、噪声加大等情况, 虽然无法完美地重现这些在拍摄过程中丢失的信息, 但可以通过一些方法来尽可能地对丢失的信息进行弥补。

     高光区域处理流程图：

     ![img](../images/4.png)

     其中，高通滤波器负责设置相应的滤波阈值，将图像中低于阈值的像素点亮度置为0，过滤掉不必要的噪声：

     ![img](../images/5.png)

     由于提取出来的阈值图像为二值图像, 在显示中高光区域会与周围区域产生分离, 也就是说容易出现伪影或块效应. 而从光线传输的客观原理以及人的直观感觉来看, 现实世界中的光源所发出的光不会直接截断, 而是随着放射距离变长而出现亮度的衰减. 与此同时, 人眼对于高亮度物体更为敏感, 进一步增强后的图像中高光区域像素点的亮度值会影响周围低亮度值的像素点的细节表现.，所以此处需要引入腐蚀操作：

     ![img](../images/6.png)

     之后再将腐蚀后的图像输入到高斯滤波器.高斯滤波器是一类根据高斯函数选择权值的线性平滑滤波器. 二维高斯函数对阈值图像有模糊的效果, 其模糊效果能够有效地模拟光线的衰减情况和去除部分噪声, 而腐蚀操作将会减少高光区域对周围像素点的遮盖效果：

     ![img](../images/7.png)

     ![img](../images/8.png)


   * 图像融合

     经过前面两部分的处理以后还需要将处理得到的图像以及色度分量图进行融合, 得到最终结果. 图像通过反色调映射的全局映射以及高光区域的局部调整后, 在中高亮度部分的表现比较优秀 , 但在暗区域部分并没有很好地保持其原有亮度, 在减小画面对比度的同时还引入了一些新的噪声. 所以在进行图像融合的时候需要对图像的暗区域进行处理：

     ![img](../images/9.png)


   * 总结

     ![img](../images/10.png)


2. 算法的效果展示：

   原图：

   ![img](../images/12.jpg)


   处理后图：

   ![img](../images/11.png)

   ​

## 二、 使用快速双边滤波进行HDR算法速度优化

基于快速双边滤波算法，对当前已经实现的HDR算法进行速度上的优化，以保证其在接下来处理视频时，每一帧的渲染时间足够短，使得视频的播放帧数能够维持在每秒几十帧的优秀帧率上。

在保留细节的同时降低对比度。它基于图像的双尺度分解为基础层，编码大规模变化和细节层。只有基层的对比度降低，从而保留了细节。使用称为双边滤波器的边缘保留滤波器获得基础层。这是一个非线性滤波器，其中每个像素的权重是使用空间域中的高斯乘以强度域中的影响函数来计算的，该影响函数减小具有大强度差异的像素的权重。我们在稳健统计框架中表达双边滤波，并展示它与各向异性扩散的关系。然后，我们通过在强度域中使用分段线性近似和适当的子采样来加速双边滤波。这导致加速两个数量级。该方法速度快，无需参数设置。

快速双边滤波算法步骤：

1. 首先他将原始的HDR数据分解成两个层：base layer 和 detail layer,然后降低base layer的对比度，不改变detail layer的数据，其中：base layer的数据用HDR原始数据进行对数操作，获取log layer，然后通过快速双边滤波算法获取，detail layer的数据通过求取log layer与base layer之间的像素值差，保留细节层。


2. 最终通过设置一定的compression factor，将detail layer和base layer进行合并，通过反对数运算，生成HDR处理结果图。

   ![img](../images/12.png)



分段线性快速双边滤波过程：

1. 我们将原始图像内的离散化的像素点亮度序列设置为NB SEGMENT，为每个这样的值计算线性滤波器，结合FFT傅里叶变换，为每个像素点计算其滤波之后的像素点值，过程也使用了较为简单的线性插值，结合像素点周围的其余点强度，生成滤波之后的log layer

   ![img](../images/13.png)

2. 为了进一步优化滤波过程的时间，上述代码中也使用到了下采样操作，对于一个样值序列间隔几个样值取样一次，这样得到新序列就是原序列的下采样。



算法速度提升效果：

![img](../images/14.png)



## 三、 HDR算法处理视频的优化过程

1. 结合Brust Photography进行高动态场景视频的多帧拟合

   Google Search实验室根据成像系统的特性，为高动态范围的视频处理提出了一种改进算法，此算法可适用于多种不同复杂环境下，并且成像质量优于传统算法。

   * 直接获取Bayer raw图像进行处理，而不是经过ISP后处理的RGB图像，从而获取更多的图像信息； 
   * 提出了新的基于FFT的配准方法，以及基于图像对的维纳滤波融合方法，使得融合结果对运动和噪声更加鲁棒


   为了处理高动态范围的多帧场景，文章提出的算法步骤为：

   1. 使用多帧图像进行阴影区域的降噪
   2. 使用局部色调映射压缩动态范围
   3. 使用高斯金字塔，将相邻帧之间的图像进行分层全局配准
      4. 实现全局去噪，同时基于图像对对时域进行融合

![img](../images/15.png)



* 仅仅做图像全局配准是不够的，因为各帧之间可能存在非刚体变换，场景移动，光线变化等，在做图像融合时还需要把这些因素考虑进去，防止鬼影的出现。对此，文章提出了基于图像对的频域时间滤波的融合方法，将参考图像中的每一个块，与其它每一帧的对应的块进行融合。对于Bayer raw输入的颜色平面使用16×1616×16的块，而对于特别暗的场景，低频噪声比较剧烈，则使用32×32大小的块。

  ​

* 融合操作是作用于图像块的空间频域的。对于一个给定的参考图像块，从多帧图像中选取其对应的图像块，每帧一块，计算他们各自的2D DFT记为$T_z(ω)$，其中$ω=(ω_x,ω_y)$。 

  假设第0帧为参考帧，为了增加对鬼影的鲁棒性，采用了如下的融合方式：

  $$\tilde{T_0}(\omega)=\frac{1}{N}\sum_{z=0}^{n-1}T_z(\omega)+A_z(\omega)[T_0(\omega)-T_z(\omega)]$$

   对于给定的频率，Az控制着第zz帧融合进最终结果的程度，其定义为经典的维纳滤波的一种变体：

  $$A_z(\omega)=\frac{|D_z(\omega)|^2}{|D_z(\omega)|^2+c\sigma^2}$$

   

2. 使用亮度均值调整算法避免视频闪烁情况发生
  结合了上述基于HDR算法，采用多帧融合视频内部每一帧的优化算法之后，在视频渲染播放时，往往会出现视频闪烁情况。此种情况的产生，大部分是由于场景与场景之间的亮度突变，导致视频播放时出现帧闪烁，这对于用户的观看体验是很不友好的，于是以下根据全局亮度均值调整算法，来避免视频闪烁情况的发生。

   * 亮度均值调整算法的思想是：结合之前几帧亮度的均值来调整当前帧的亮度。使得同一场景中，帧与帧之间亮度差值维持在*人眼不可察觉的最大闪烁亮度差*之间，改善转换之后视频闪烁的问题。

     ​

   * 全局亮度均值调整算法步骤为：

      0. 确定调整步长为$Step$，接下来根据步长进行帧场景之间的亮度调整
      
      1. 调整步长内的视频帧进行HDR算法与Brust多帧拟合，计算并保存每帧的平均亮度值 $L_i$;

      2. 处理第n帧前，计算此前步长内的的平均亮度$L_{n avg} = \frac{1}{Step}\sum^{n-1}_{i =n-Step}L_i$

      3. 对第n帧进行改进后的HDR算法处理，并计算得到`HDR`帧的平均亮度值$L_{n0}$

      4. 如果$|L_{n0}- L_{navg}| \leq \varepsilon _{L}$，其中$\varepsilon_L$ 表示人眼不可察觉最大闪烁亮度差，则不对该帧亮度进行调整，转到步骤(2)继续处理下一帧。
      如果$|L_{n0}- L_{navg}| \geq \varepsilon _{H}$，其中$\varepsilon_H$表示判断场景改变的的依据，则视频进入了新的场景，不对该帧进行处理，转到步骤(1)，初始化。
      如果$\varepsilon_L < |L{n0}- L_{navg}| < \varepsilon_H$，则代表在同一场景中发生了闪烁现象，调整该帧的亮度值得到$L{n1}$，$|L_{n1}- L_{navg}| \leq \varepsilon _{L}$，转到步骤(2)继续处理下一帧。

3. 使用 Leaky Integrator 避免视频闪烁情况发生

   由于视频帧序列中可能出现异常的像素值，帧之间的参数可能出现过大的改变导致可见的闪烁现象，因此可以使用漏积分器（Leaky Integrator）来对帧中与像素值相关的参数进行调整，使得对每一帧的处理可以保留先前帧的参数影响，以降低帧之间的剧烈变化，减少闪烁现象:
    ​ $$A_n=(1-\alpha_L)A_{(n-1)}+\alpha_LA​$$

   其中$\alpha_L\in[0, 1] $为定义的常数。经过漏积分器处理后可大幅降低参数的变化幅度，从而减少闪烁：

   ![](../images/leaky-integrator.png)

   上图中红色曲线表示的为未进行处理时的$A=L_{max}-L_{avg}$的数值，蓝色曲线为$A_n=(1-\alpha_L)A_{(n-1)}+\alpha_LA$的数值。

4. 使用多核 CPU 并行化实现实时的视频帧处理

   在我们的实验中，经过算法处理的 HDR 视频播放器在单线程下只能获得20到24的帧率，达不到实时性的要求，对于播放器而言并不能获得良好的体验，因此需要通过其他方法提升速度，以获得更高的帧率。

   在相关的许多论文中，视频的实时 HDR 处理大部分使用的是 FPGA 或 GPU 等特殊硬件进行，这些硬件的特点是特别适用于并行计算，但是这些方法需要使用特殊硬件，成本较高而且编程难度较大。考虑到目前多核 CPU 的性能已可满足许多计算要求，本次的实现选择使用 CPU 并行化来加速视频处理，实现实时的视频播放。

   考虑到多线程编程的内存安全与速度等因素，我们选用了 Rust 语言来实现视频帧处理的算法，将代码编译为动态链接库供由 C++ 编写的视频播放器调用。

   并行化的主要实现位置为在对视频每帧的处理的各个步骤对图像像素进行并行的处理。经过实验，在一台搭载8核 CPU 下的计算机中进行并行化处理后的视频播放帧率可以达到60至70帧，达到较好的实时性。


## 四、 实现全景VR视频播放器
框架结构：

- FFMPEG：媒体处理，输出YUV444图像（传给hdr部分进行处理）
- OpenGL：创建球体模型，贴图，渲染
- GLFW：创建显示窗口，处理鼠标键盘输入

首先需要对读取的视频进行解码，这里选择使用FFMPEG库。FFMPEG库提供了许多有用的函数帮助进行视频的解码，格式转换等等，其中有这些类型在解码过程中最为重要：

```
AVFormatContext: FFMPEG解封装(flv, mp4, rmvb, avi)功能的结构体
AVCodecContext: 主要的外部API结构体，用在编解码过程中
AVStream: 存储每个视频/音频流信息的结构体
AVCodec: 存储编解码器信息的结构体
AVFrame: 一般用于存储音视频的（非压缩）原始数据，以及其他一些相关信息
AVPacket: 存储压缩编码数据相关信息的结构体
```

循环中每次读取一帧画面（这里以YUV444格式读取），此时可以得到y，u，v三个通道的数据，将它们传送给hdr处理部分接口，得到处理之后的数据，并将对应通道数据保存至自定义结构体中，以便渲染时使用。



OpenGL渲染部分最为重要的就是球体的渲染，因为最后需要将解码出的画面渲染到球体上。球体自然需要通过顶点生成，但如果全靠顶点可能需要的数量过大，这里可以通过少量顶点连接成的三角形来模拟球体，顶点越多球体当然就越平滑，但同时也会影响渲染性能。这里使用81×81（一层81个，共81层）个顶点来实现一个球体，球体每一层的半径以及顶点的坐标可以通过三角函数计算出来。

最后，通过添加对鼠标事件的相应处理使得可以通过移动鼠标来改变视角，做到自由360°观看全景视频的效果。

实现的球体效果（外部视角）：

![](../images/spere.png)



## 五、 实验总结

1. 本次VR项目实际上难度较大，前前后后通过研读了很多论文，研究了众多算法，才得以将算法落到实地，成功实现高效率的实时HDR渲染的全景视频播放器。


2. 本次作业也是充分融合了VR课程的知识点，结合了老师提供的各种论文，最终通过参考此些论文，成功完成项目，小组内的组员也是经历了很久的算法实现和测试，最终完成了VR课程的期末项目。




## 六、参考文献

[1] 朱恩弘,张红英,吴亚东,霍永青.单幅图像的高动态范围图像生成方法[J].计算机辅助设计与图形学学报,2016,28(10):1713-1722.

[2] Sam Hasinoff, Dillon Sharlet, Ryan Geiss, Andrew Adams, Jonathan T. Barron, Florian Kainz, Jiawen Chen, and Marc Levoy. Burst photography for high dynamic range and low-light imaging on mobile cameras. SIGGRAPH Asia, 2016.

[3] Durand, Frédo, and Julie Dorsey. "Fast bilateral filtering for the display of high-dynamic-range images." ACM transactions on graphics (TOG). Vol. 21. No. 3. ACM, 2002.

[4] Benjamin Guthier, Stephan Kopf, and Wolfgang Effelsberg. Algorithms for a real-time hdr video system. Pattern Recognition Letters, 34(1):25–33, 2013.

[5] Gabriel Eilertsen, Rafał K Mantiuk, and Jonas Unger. A comparative review of tone-mapping algorithms for high dynamic range video. In Computer Graphics Forum, volume 36, pages 565–592. Wiley Online Library, 2017.

[6] Benjamin Guthier, Stephan Kopf, Marc Eble, and Wolfgang Effelsberg. Flicker reduction in tone mapped high dynamic range video. In Color Imaging: Displaying, Processing, Hardcopy, and Applications, 2011.

[7] Chris Kiser, Erik Reinhard, Mike Tocci, and Nora Tocci. Real time automated tone mapping system for hdr video. In IEEE International Conference on Image Processing, volume 134. IEEE Orlando, FL, 2012.
